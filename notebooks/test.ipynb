{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/surkov/conda_envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hook(m,i,o):\n",
    "    print(o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0653, -0.2262,  0.0242,  ..., -0.0565,  0.0307,  0.2187],\n",
      "         [ 0.0558,  0.0318, -0.0228,  ..., -0.2404, -0.0032,  0.0370],\n",
      "         [-0.0338, -0.0301, -0.0295,  ...,  0.1639, -0.0569,  0.0554],\n",
      "         ...,\n",
      "         [ 0.0487,  0.0923, -0.0801,  ..., -0.1878, -0.0219, -0.1405],\n",
      "         [ 0.0523,  0.0016,  0.0901,  ...,  0.1566,  0.0726,  0.0215],\n",
      "         [-0.2051,  0.0834,  0.1736,  ...,  0.0523,  0.0012,  0.3493]]],\n",
      "       device='cuda:0')\n",
      "<pad> Même s'il y a d'autres histoires d'histoires d'histoires d'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "input_text = \"translate into french: Tell me about the greatest scientist in the world\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "base_model.get_submodule(\"encoder.block.5.layer.0.SelfAttention.q\").register_forward_hook(print_hook)\n",
    "\n",
    "outputs = base_model.generate(input_ids, max_new_tokens=30)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = dict(target_modules=[\".q\", \".v\"], alpha=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.SelfAttention.q\n",
      "encoder.block.0.layer.0.SelfAttention.v\n",
      "encoder.block.1.layer.0.SelfAttention.q\n",
      "encoder.block.1.layer.0.SelfAttention.v\n",
      "encoder.block.2.layer.0.SelfAttention.q\n",
      "encoder.block.2.layer.0.SelfAttention.v\n",
      "encoder.block.3.layer.0.SelfAttention.q\n",
      "encoder.block.3.layer.0.SelfAttention.v\n",
      "encoder.block.4.layer.0.SelfAttention.q\n",
      "encoder.block.4.layer.0.SelfAttention.v\n",
      "encoder.block.5.layer.0.SelfAttention.q\n",
      "encoder.block.5.layer.0.SelfAttention.v\n",
      "encoder.block.6.layer.0.SelfAttention.q\n",
      "encoder.block.6.layer.0.SelfAttention.v\n",
      "encoder.block.7.layer.0.SelfAttention.q\n",
      "encoder.block.7.layer.0.SelfAttention.v\n",
      "encoder.block.8.layer.0.SelfAttention.q\n",
      "encoder.block.8.layer.0.SelfAttention.v\n",
      "encoder.block.9.layer.0.SelfAttention.q\n",
      "encoder.block.9.layer.0.SelfAttention.v\n",
      "encoder.block.10.layer.0.SelfAttention.q\n",
      "encoder.block.10.layer.0.SelfAttention.v\n",
      "encoder.block.11.layer.0.SelfAttention.q\n",
      "encoder.block.11.layer.0.SelfAttention.v\n",
      "encoder.block.12.layer.0.SelfAttention.q\n",
      "encoder.block.12.layer.0.SelfAttention.v\n",
      "encoder.block.13.layer.0.SelfAttention.q\n",
      "encoder.block.13.layer.0.SelfAttention.v\n",
      "encoder.block.14.layer.0.SelfAttention.q\n",
      "encoder.block.14.layer.0.SelfAttention.v\n",
      "encoder.block.15.layer.0.SelfAttention.q\n",
      "encoder.block.15.layer.0.SelfAttention.v\n",
      "encoder.block.16.layer.0.SelfAttention.q\n",
      "encoder.block.16.layer.0.SelfAttention.v\n",
      "encoder.block.17.layer.0.SelfAttention.q\n",
      "encoder.block.17.layer.0.SelfAttention.v\n",
      "encoder.block.18.layer.0.SelfAttention.q\n",
      "encoder.block.18.layer.0.SelfAttention.v\n",
      "encoder.block.19.layer.0.SelfAttention.q\n",
      "encoder.block.19.layer.0.SelfAttention.v\n",
      "encoder.block.20.layer.0.SelfAttention.q\n",
      "encoder.block.20.layer.0.SelfAttention.v\n",
      "encoder.block.21.layer.0.SelfAttention.q\n",
      "encoder.block.21.layer.0.SelfAttention.v\n",
      "encoder.block.22.layer.0.SelfAttention.q\n",
      "encoder.block.22.layer.0.SelfAttention.v\n",
      "encoder.block.23.layer.0.SelfAttention.q\n",
      "encoder.block.23.layer.0.SelfAttention.v\n",
      "decoder.block.0.layer.0.SelfAttention.q\n",
      "decoder.block.0.layer.0.SelfAttention.v\n",
      "decoder.block.0.layer.1.EncDecAttention.q\n",
      "decoder.block.0.layer.1.EncDecAttention.v\n",
      "decoder.block.1.layer.0.SelfAttention.q\n",
      "decoder.block.1.layer.0.SelfAttention.v\n",
      "decoder.block.1.layer.1.EncDecAttention.q\n",
      "decoder.block.1.layer.1.EncDecAttention.v\n",
      "decoder.block.2.layer.0.SelfAttention.q\n",
      "decoder.block.2.layer.0.SelfAttention.v\n",
      "decoder.block.2.layer.1.EncDecAttention.q\n",
      "decoder.block.2.layer.1.EncDecAttention.v\n",
      "decoder.block.3.layer.0.SelfAttention.q\n",
      "decoder.block.3.layer.0.SelfAttention.v\n",
      "decoder.block.3.layer.1.EncDecAttention.q\n",
      "decoder.block.3.layer.1.EncDecAttention.v\n",
      "decoder.block.4.layer.0.SelfAttention.q\n",
      "decoder.block.4.layer.0.SelfAttention.v\n",
      "decoder.block.4.layer.1.EncDecAttention.q\n",
      "decoder.block.4.layer.1.EncDecAttention.v\n",
      "decoder.block.5.layer.0.SelfAttention.q\n",
      "decoder.block.5.layer.0.SelfAttention.v\n",
      "decoder.block.5.layer.1.EncDecAttention.q\n",
      "decoder.block.5.layer.1.EncDecAttention.v\n",
      "decoder.block.6.layer.0.SelfAttention.q\n",
      "decoder.block.6.layer.0.SelfAttention.v\n",
      "decoder.block.6.layer.1.EncDecAttention.q\n",
      "decoder.block.6.layer.1.EncDecAttention.v\n",
      "decoder.block.7.layer.0.SelfAttention.q\n",
      "decoder.block.7.layer.0.SelfAttention.v\n",
      "decoder.block.7.layer.1.EncDecAttention.q\n",
      "decoder.block.7.layer.1.EncDecAttention.v\n",
      "decoder.block.8.layer.0.SelfAttention.q\n",
      "decoder.block.8.layer.0.SelfAttention.v\n",
      "decoder.block.8.layer.1.EncDecAttention.q\n",
      "decoder.block.8.layer.1.EncDecAttention.v\n",
      "decoder.block.9.layer.0.SelfAttention.q\n",
      "decoder.block.9.layer.0.SelfAttention.v\n",
      "decoder.block.9.layer.1.EncDecAttention.q\n",
      "decoder.block.9.layer.1.EncDecAttention.v\n",
      "decoder.block.10.layer.0.SelfAttention.q\n",
      "decoder.block.10.layer.0.SelfAttention.v\n",
      "decoder.block.10.layer.1.EncDecAttention.q\n",
      "decoder.block.10.layer.1.EncDecAttention.v\n",
      "decoder.block.11.layer.0.SelfAttention.q\n",
      "decoder.block.11.layer.0.SelfAttention.v\n",
      "decoder.block.11.layer.1.EncDecAttention.q\n",
      "decoder.block.11.layer.1.EncDecAttention.v\n",
      "decoder.block.12.layer.0.SelfAttention.q\n",
      "decoder.block.12.layer.0.SelfAttention.v\n",
      "decoder.block.12.layer.1.EncDecAttention.q\n",
      "decoder.block.12.layer.1.EncDecAttention.v\n",
      "decoder.block.13.layer.0.SelfAttention.q\n",
      "decoder.block.13.layer.0.SelfAttention.v\n",
      "decoder.block.13.layer.1.EncDecAttention.q\n",
      "decoder.block.13.layer.1.EncDecAttention.v\n",
      "decoder.block.14.layer.0.SelfAttention.q\n",
      "decoder.block.14.layer.0.SelfAttention.v\n",
      "decoder.block.14.layer.1.EncDecAttention.q\n",
      "decoder.block.14.layer.1.EncDecAttention.v\n",
      "decoder.block.15.layer.0.SelfAttention.q\n",
      "decoder.block.15.layer.0.SelfAttention.v\n",
      "decoder.block.15.layer.1.EncDecAttention.q\n",
      "decoder.block.15.layer.1.EncDecAttention.v\n",
      "decoder.block.16.layer.0.SelfAttention.q\n",
      "decoder.block.16.layer.0.SelfAttention.v\n",
      "decoder.block.16.layer.1.EncDecAttention.q\n",
      "decoder.block.16.layer.1.EncDecAttention.v\n",
      "decoder.block.17.layer.0.SelfAttention.q\n",
      "decoder.block.17.layer.0.SelfAttention.v\n",
      "decoder.block.17.layer.1.EncDecAttention.q\n",
      "decoder.block.17.layer.1.EncDecAttention.v\n",
      "decoder.block.18.layer.0.SelfAttention.q\n",
      "decoder.block.18.layer.0.SelfAttention.v\n",
      "decoder.block.18.layer.1.EncDecAttention.q\n",
      "decoder.block.18.layer.1.EncDecAttention.v\n",
      "decoder.block.19.layer.0.SelfAttention.q\n",
      "decoder.block.19.layer.0.SelfAttention.v\n",
      "decoder.block.19.layer.1.EncDecAttention.q\n",
      "decoder.block.19.layer.1.EncDecAttention.v\n",
      "decoder.block.20.layer.0.SelfAttention.q\n",
      "decoder.block.20.layer.0.SelfAttention.v\n",
      "decoder.block.20.layer.1.EncDecAttention.q\n",
      "decoder.block.20.layer.1.EncDecAttention.v\n",
      "decoder.block.21.layer.0.SelfAttention.q\n",
      "decoder.block.21.layer.0.SelfAttention.v\n",
      "decoder.block.21.layer.1.EncDecAttention.q\n",
      "decoder.block.21.layer.1.EncDecAttention.v\n",
      "decoder.block.22.layer.0.SelfAttention.q\n",
      "decoder.block.22.layer.0.SelfAttention.v\n",
      "decoder.block.22.layer.1.EncDecAttention.q\n",
      "decoder.block.22.layer.1.EncDecAttention.v\n",
      "decoder.block.23.layer.0.SelfAttention.q\n",
      "decoder.block.23.layer.0.SelfAttention.v\n",
      "decoder.block.23.layer.1.EncDecAttention.q\n",
      "decoder.block.23.layer.1.EncDecAttention.v\n"
     ]
    }
   ],
   "source": [
    "from multilora import LoRAModel\n",
    "model = LoRAModel(base_model, lora_config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0653, -0.2262,  0.0242,  ..., -0.0565,  0.0307,  0.2187],\n",
      "         [ 0.0558,  0.0318, -0.0228,  ..., -0.2404, -0.0032,  0.0370],\n",
      "         [-0.0338, -0.0301, -0.0295,  ...,  0.1639, -0.0569,  0.0554],\n",
      "         ...,\n",
      "         [ 0.0487,  0.0923, -0.0801,  ..., -0.1878, -0.0219, -0.1405],\n",
      "         [ 0.0523,  0.0016,  0.0901,  ...,  0.1566,  0.0726,  0.0215],\n",
      "         [-0.2051,  0.0834,  0.1736,  ...,  0.0523,  0.0012,  0.3493]]],\n",
      "       device='cuda:0')\n",
      "<pad> Même s'il y a d'autres histoires d'\n"
     ]
    }
   ],
   "source": [
    "model.adapter_ids.data = torch.tensor([0]).to('cuda')\n",
    "outputs = model.base_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = torch.load('adapter_model.bin', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight', 'base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight', 'base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight', 'base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight', 'base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight', 'base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight', 'base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight', 'base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight', 'base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight\n",
      "torch.Size([1024, 16])\n",
      "base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight\n",
      "torch.Size([16, 1024])\n",
      "base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight\n",
      "torch.Size([1024, 16])\n"
     ]
    }
   ],
   "source": [
    "for key, value in file.items():\n",
    "    if key.endswith('lora_A.weight'):\n",
    "        aorb = 'A'\n",
    "        stem = key.strip('.lora_A.weight')\n",
    "    else:\n",
    "        aorb = 'B'\n",
    "        stem = key.strip('.lora_B.weight')\n",
    "    path = stem + '.module2.' + aorb\n",
    "    path = path.replace('.model.', '.')\n",
    "    print(key)\n",
    "    print(value.shape)\n",
    "    model.get_parameter(path).data = value.T.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.6186e-02, -2.2885e-01,  2.4074e-02,  ..., -5.1724e-02,\n",
      "           2.8821e-02,  2.1549e-01],\n",
      "         [ 5.3973e-02,  3.7252e-02, -2.8463e-02,  ..., -2.3661e-01,\n",
      "          -4.8198e-03,  2.3165e-02],\n",
      "         [-3.3200e-02, -3.1014e-02, -3.0228e-02,  ...,  1.6477e-01,\n",
      "          -5.7422e-02,  5.5504e-02],\n",
      "         ...,\n",
      "         [ 4.8742e-02,  9.2203e-02, -8.0042e-02,  ..., -1.8773e-01,\n",
      "          -2.1930e-02, -1.4047e-01],\n",
      "         [ 5.1625e-02, -1.1196e-04,  8.8337e-02,  ...,  1.5608e-01,\n",
      "           7.2920e-02,  2.4301e-02],\n",
      "         [-2.0471e-01,  8.1783e-02,  1.7320e-01,  ...,  5.3410e-02,\n",
      "          -7.8545e-06,  3.4848e-01]]], device='cuda:0')\n",
      "<pad> Même en Allemagne, il n'y a pas de l'\n"
     ]
    }
   ],
   "source": [
    "model.adapter_ids.data = torch.tensor([0]).to('cuda')\n",
    "outputs = model.base_model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
